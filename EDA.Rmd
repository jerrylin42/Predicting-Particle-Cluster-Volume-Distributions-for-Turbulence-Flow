---
title: "EDA"
author: "Particle Clustering Proj Group"
date: "`10/17/2025`"
output: pdf_document
---

```{r}
library(tidyverse)

train_data <- read_csv("data-train-processed.csv")


#Fix the infinity values in Fr via inverse logit transformation, but keep Fr as numeric
train_data$Fr <- as.numeric(tolower(as.character(train_data$Fr)))
train_data$Fr_invlogit <- 1 / (1 + exp(-train_data$Fr))


#Re only has 3 distinct values, change to categorical var
train_data$Re <- as.factor(train_data$Re)
train_data |> distinct(Re)

train_data
```

```{r}
library(GGally)
ggpairs(
  train_data |> select(St, Re, Fr_invlogit, mean, variance, skewness, kurtosis)
) 
```

```{r}
#check individual distributions of variables for more detail 
hist(train_data$St, breaks=20)
hist(train_data$Fr_invlogit, breaks=20)
hist(train_data$mean, breaks=20)
hist(train_data$variance, breaks=20)
hist(train_data$skewness, breaks=20)
hist(train_data$kurtosis, breaks=20)
```

Immediate Notes

-   Collinearity between the 3 predictors is not a large concern

-   log transformation on mean/var/skewness/kurtosis will be very helpful for reducing the impact of right skew on the response variable. Recommended log transform on St and (potentially Fr_invlogit).

```{r}

basicmean_lm <- lm(mean ~ St + Re + Fr_invlogit, data = train_data)
basicvar_lm  <- lm(variance ~ St + Re + Fr_invlogit, data = train_data)
basicskew_lm <- lm(skewness ~ St + Re + Fr_invlogit, data = train_data)
basickurt_lm <- lm(kurtosis ~ St + Re + Fr_invlogit, data = train_data)

summary(basicmean_lm)
summary(basicvar_lm)
summary(basicskew_lm)
summary(basickurt_lm)

```

Observations: mean performs well as just a base model. The rest are subpar to okay going off of R\^2.

### Recommendations for workflow:


For EACH model:

Start with full model, including all variables with log-transformations and their interaction effects

Try multi-linear regression by running the full model through best-subset selection (should be ok, we don't have many variables) and use CV or AIC as the evaluation metric. We did this in a lab

Pick a best candidate model from subset selection

Then, run that model with those variables except change it from regular OLS to ridge (Let's not try lasso because we don't have many predictors already, but it can work still)

Compared the OLS model to the ridge model using CV.
