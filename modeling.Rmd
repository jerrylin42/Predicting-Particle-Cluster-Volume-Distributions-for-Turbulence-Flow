---
title: "Predictive Modeling for Particle Cluster Distributions"
author: "Particle Clustering Proj Group"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Overview

**Goal**: Predict 4 summary statistics of particle cluster volume distributions

- **Mean** ($\mu$): Average cluster volume
- **Variance** ($\sigma^2$): Variability in cluster sizes  
- **Skewness** ($\gamma$): Asymmetry of distribution
- **Kurtosis** ($\kappa$): Tail heaviness

**Modeling Pipeline** for each response:

1. Apply log transformations to handle right skew
2. Fit full model with 2-way interactions
3. **Best subset selection + OLS** → Variable selection via BIC, fit OLS on selected variables
4. **LASSO on full model** → Automatic variable selection + shrinkage via regularization
5. **Compare via 10-fold CV** → Select method with lowest CV RMSE
6. Generate predictions with **confidence intervals** to quantify uncertainty

**Methodological Note:** We compare subset selection (traditional) vs LASSO (modern regularization). LASSO is run on the full model to allow it to perform its own variable selection, rather than applying it to already-selected variables (which would be redundant).

**Important Note on Re:** Although Re only has 3 observed values (90, 224, 398), it is treated as **continuous** to allow prediction at any Reynolds number. However, this introduces substantial uncertainty, which we quantify using 95% confidence and prediction intervals.

---

# 1. Setup

## 1.1 Load Packages

```{r packages}
library(tidyverse)
library(leaps)      # best subset selection
library(glmnet)     # LASSO regression
set.seed(325)       # reproducibility
```

## 1.2 Load Data

```{r load_data}
train <- read_csv("data-train-processed.csv")
cat("# observations:", nrow(train))
```

## 1.3 Transform Predictors

```{r transform_predictors}
train <- train %>%
  mutate(
    # Treating as continuous allows prediction at any Re value
    # Note: High uncertainty expected due to limited observed values
    Re = as.numeric(Re),
    
    # Fr: Inverse logit (handles Inf values)
    Fr_invlogit = 1 / (1 + exp(-as.numeric(Fr))),
    
    # St: Log transform (right-skewed)
    log_St = log(St)
  )
```

## 1.4 Transform Responses

```{r transform_responses}
#log
train <- train %>%
  mutate(
    log_mean = log(mean),
    log_variance = log(variance),
    log_skewness = log(skewness),
    log_kurtosis = log(kurtosis)
  )
```

## 1.5 Helper Functions

```{r helper_cv}
# 10-fold cross-validation RMSE
cv_rmse <- function(data, formula, k = 10) {
  # Create equal-sized folds
  n <- nrow(data)
  fold_ids <- sample(rep(1:k, length.out = n))
  data$fold <- fold_ids
  
  cv_errors <- numeric(k)
  
  for(i in 1:k) {
    train_fold <- data[data$fold != i, ]
    test_fold <- data[data$fold == i, ]
    
    model <- lm(formula, data = train_fold)
    preds <- predict(model, newdata = test_fold)
    
    response <- all.vars(formula)[1]
    cv_errors[i] <- mean((test_fold[[response]] - preds)^2)
  }
  
  return(sqrt(mean(cv_errors)))
}
```

```{r helper_subset}
# top models from regsubsets
show_top_models <- function(regfit, n = 5) {
  summ <- summary(regfit)
  data.frame(
    n_vars = 1:length(summ$bic),
    BIC = summ$bic,
    adj_R2 = summ$adjr2
  ) %>% 
    arrange(BIC) %>% 
    head(n)
}
```

---

# 2. Model 1: MEAN

## 2.1 Fit Full Model

```{r mean_full}
cat("=== MODELING MEAN ===\n\n")

#full model
formula_mean_full <- log_mean ~ log_St + Re + Fr_invlogit + log_St:Re + log_St:Fr_invlogit + Re:Fr_invlogit

lm_mean_full <- lm(formula_mean_full, data = train)
cat("Full model R²:", summary(lm_mean_full)$r.squared, "\n")
cat("Full model variables:", length(coef(lm_mean_full)) - 1)
```
A full multiple linear regression model was fitted to predict log_mean using the three predictors (log_St, Re, and Fr_invlogit) and all pairwise interaction terms. The resulting model achieved an $R^2$ of 99.75% with 9 model variables, indicating an excellent fit to the training data and suggesting that the predictors capture almost all variation in the mean cluster volume. The inclusion of interaction terms is physically meaningful, as particle inertia effects may depend on turbulence and gravity. However, such a high $R^2$ could suggest overfitting, so subsequent steps will use cross-validation and subset selection to ensure generalizability.

## 2.2 Best Subset Selection

```{r mean_subset}
# subset selection
regfit_mean <- regsubsets(formula_mean_full, data = train, nvmax = 10, method = "exhaustive")

# top 5 models
cat("Top 5 models by BIC:\n")
print(show_top_models(regfit_mean, n = 5))

# Extract best model (lowest BIC)
best_models_mean <- show_top_models(regfit_mean)
best_size <- best_models_mean$n_vars[1]
best_vars <- names(coef(regfit_mean, best_size))[-1]  # Remove intercept

cat("Best model has", best_size, "variables\n")
cat("Variables:", paste(best_vars, collapse = ", "))
```

Best subset selection identified a 6-variable model as optimal based on the BIC score, achieving an adjusted $R^2$ of 99.71%, which is nearly identical to the full model’s performance but with fewer terms. This indicates that some interaction terms are unnecessary and that the simplified model provides a better balance between accuracy and interpretability, reducing the risk of overfitting.


## 2.3 Build Best Model Formula

```{r mean_formula}
#create formula to load
formula_mean_best <- as.formula(paste("log_mean ~", paste(best_vars, collapse = " + ")))
cat("best model formula:\n")
print(formula_mean_best)
```

The 6 variables reported by best subset selection include dummy variables automatically created for the categorical predictor `Re`. Once these are collapsed back into the original factor, the final model consists of 4 unique predictors (`log_St`, `Re`, `Fr_invlogit`, and `Re:Fr_invlogit`).

## 2.4 Compare OLS vs LASSO

```{r mean_compare}
# Fit OLS on best subset variables
lm_mean_best <- lm(formula_mean_best, data = train)
cv_ols_mean <- cv_rmse(train, formula_mean_best, k = 10)
cat("OLS (subset) CV RMSE:", cv_ols_mean, "\n\n")

# Fit LASSO on FULL model (all variables for automatic selection)
X_mean_full <- model.matrix(formula_mean_full, data = train)[, -1]
y_mean <- train$log_mean
lasso_mean <- cv.glmnet(X_mean_full, y_mean, alpha = 1, nfolds = 10)
cv_lasso_mean <- min(sqrt(lasso_mean$cvm))
cat("LASSO (full) CV RMSE:", cv_lasso_mean, "\n")
cat("Best lambda:", lasso_mean$lambda.min, "\n")
cat("Non-zero coefficients:", sum(coef(lasso_mean, s = "lambda.min") != 0) - 1, "\n")
```

## 2.5 Select Final Model

```{r mean_final}
if(cv_ols_mean < cv_lasso_mean) {
  cat("FINAL MODEL: OLS on subset (lower CV RMSE)\n")
  final_model_mean <- lm_mean_best
  use_lasso_mean <- FALSE
} else {
  cat("FINAL MODEL: LASSO on full model (lower CV RMSE)\n")
  final_model_mean <- lasso_mean
  use_lasso_mean <- TRUE
}
```
The comparison shows which variable selection approach performs better: explicit subset selection via BIC (OLS) versus regularization-based automatic selection (LASSO). LASSO is evaluated on the full model to allow it to perform its own variable selection through coefficient shrinkage.
---

# 3. Model 2: VARIANCE

## 3.1 Fit Full Model

```{r var_full}
cat("=== MODELING VARIANCE ===\n\n")


#full model
formula_var_full <- log_variance ~ log_St + Re + Fr_invlogit + log_St:Re + log_St:Fr_invlogit + Re:Fr_invlogit

lm_var_full <- lm(formula_var_full, data = train)
cat("Full model R²:", summary(lm_var_full)$r.squared, "\n")
cat("Full model variables:", length(coef(lm_var_full)) - 1)
```

## 3.2 Best Subset Selection

```{r var_subset}
# subset selection
regfit_var <- regsubsets(formula_var_full, data = train, nvmax = 10, method = "exhaustive")

# top 5 models
cat("Top 5 models by BIC:\n")
print(show_top_models(regfit_var, n = 5))

best_models_var <- show_top_models(regfit_var)
best_size <- best_models_var$n_vars[1]
best_vars <- names(coef(regfit_var, best_size))[-1]  # Remove intercept

cat("Best model has", best_size, "variables\n")
cat("Variables:", paste(best_vars, collapse = ", "))
```

## 3.3 Build Best Model Formula

```{r var_formula}
#formula
formula_var_best <- as.formula(paste("log_variance ~", paste(best_vars, collapse = " + ")))
cat("best model formula:\n")
print(formula_var_best)
```

## 3.4 Compare OLS vs LASSO

```{r var_compare}
# Fit OLS on best subset variables
lm_var_best <- lm(formula_var_best, data = train)
cv_ols_var <- cv_rmse(train, formula_var_best, k = 10)
cat("OLS (subset) CV RMSE:", cv_ols_var, "\n\n")

# Fit LASSO on FULL model
X_var_full <- model.matrix(formula_var_full, data = train)[, -1]
y_var <- train$log_variance
lasso_var <- cv.glmnet(X_var_full, y_var, alpha = 1, nfolds = 10)
cv_lasso_var <- min(sqrt(lasso_var$cvm))
cat("LASSO (full) CV RMSE:", cv_lasso_var, "\n")
cat("Best lambda:", lasso_var$lambda.min, "\n")
cat("Non-zero coefficients:", sum(coef(lasso_var, s = "lambda.min") != 0) - 1, "\n")
```

## 3.5 Select Final Model

```{r var_final}
if(cv_ols_var < cv_lasso_var) {
  cat("FINAL MODEL: OLS on subset (lower CV RMSE)\n")
  final_model_var <- lm_var_best
  use_lasso_var <- FALSE
} else {
  cat("FINAL MODEL: LASSO on full model (lower CV RMSE)\n")
  final_model_var <- lasso_var
  use_lasso_var <- TRUE
}
```

---

# 4. Model 3: SKEWNESS

## 4.1 Fit Full Model

```{r skew_full}
cat("=== MODELING SKEWNESS ===\n\n")

# Define full model with all 2-way interactions
formula_skew_full <- log_skewness ~ log_St + Re + Fr_invlogit + log_St:Re + log_St:Fr_invlogit + Re:Fr_invlogit

# Fit full model
lm_skew_full <- lm(formula_skew_full, data = train)
cat("Full model R²:", round(summary(lm_skew_full)$r.squared, 4), "\n")
cat("Full model variables:", length(coef(lm_skew_full)) - 1)
```

## 4.2 Best Subset Selection

```{r skew_subset}
# subset selection
regfit_skew <- regsubsets(formula_skew_full, data = train, nvmax = 10, method = "exhaustive")

# top 5 models
cat("Top 5 models by BIC:\n")
print(show_top_models(regfit_skew, n = 5))

best_models_skew <- show_top_models(regfit_skew)
best_size <- best_models_skew$n_vars[1]
best_vars <- names(coef(regfit_skew, best_size))[-1]  # Remove intercept

cat("Best model has", best_size, "variables\n")
cat("Variables:", paste(best_vars, collapse = ", "))
```

## 4.3 Build Best Model Formula

```{r skew_formula}
# Create formula for best model
formula_skew_best <- as.formula(paste("log_skewness ~", paste(best_vars, collapse = " + ")))
cat("best model formula:\n")
print(formula_skew_best)
```

## 4.4 Compare OLS vs LASSO

```{r skew_compare}
# Fit OLS on best subset variables
lm_skew_best <- lm(formula_skew_best, data = train)
cv_ols_skew <- cv_rmse(train, formula_skew_best, k = 10)
cat("OLS (subset) CV RMSE:", cv_ols_skew, "\n\n")

# Fit LASSO on FULL model
X_skew_full <- model.matrix(formula_skew_full, data = train)[, -1]
y_skew <- train$log_skewness
lasso_skew <- cv.glmnet(X_skew_full, y_skew, alpha = 1, nfolds = 10)
cv_lasso_skew <- min(sqrt(lasso_skew$cvm))
cat("LASSO (full) CV RMSE:", cv_lasso_skew, "\n")
cat("Best lambda:", lasso_skew$lambda.min, "\n")
cat("Non-zero coefficients:", sum(coef(lasso_skew, s = "lambda.min") != 0) - 1, "\n")
```

## 4.5 Select Final Model

```{r skew_final}
if(cv_ols_skew < cv_lasso_skew) {
  cat("FINAL MODEL: OLS on subset (lower CV RMSE)\n")
  final_model_skew <- lm_skew_best
  use_lasso_skew <- FALSE
} else {
  cat("FINAL MODEL: LASSO on full model (lower CV RMSE)\n")
  final_model_skew <- lasso_skew
  use_lasso_skew <- TRUE
}
```

---

# 5. Model 4: KURTOSIS

## 5.1 Fit Full Model

```{r kurt_full}
cat("=== MODELING KURTOSIS ===\n\n")

#full
formula_kurt_full <- log_kurtosis ~ log_St + Re + Fr_invlogit + log_St:Re + log_St:Fr_invlogit + Re:Fr_invlogit

lm_kurt_full <- lm(formula_kurt_full, data = train)
cat("Full model R²:", round(summary(lm_kurt_full)$r.squared, 4), "\n")
cat("Full model variables:", length(coef(lm_kurt_full)) - 1)
```

## 5.2 Best Subset Selection

```{r kurt_subset}
# subset selection
regfit_kurt <- regsubsets(formula_kurt_full, data = train, nvmax = 10, method = "exhaustive")

# top 5 models
cat("Top 5 models by BIC:\n")
print(show_top_models(regfit_kurt, n = 5))

best_models_kurt <- show_top_models(regfit_kurt)
best_size <- best_models_kurt$n_vars[1]
best_vars <- names(coef(regfit_kurt, best_size))[-1]  # Remove intercept

cat("Best model has", best_size, "variables\n")
cat("Variables:", paste(best_vars, collapse = ", "))
```

## 5.3 Build Best Model Formula

```{r kurt_formula}
# formula
formula_kurt_best <- as.formula(paste("log_kurtosis ~", paste(best_vars, collapse = " + ")))
cat("best model formula:\n")
print(formula_kurt_best)
```

## 5.4 Compare OLS vs LASSO

```{r kurt_compare}
# Fit OLS on best subset variables
lm_kurt_best <- lm(formula_kurt_best, data = train)
cv_ols_kurt <- cv_rmse(train, formula_kurt_best, k = 10)
cat("OLS (subset) CV RMSE:", cv_ols_kurt, "\n\n")

# Fit LASSO on FULL model
X_kurt_full <- model.matrix(formula_kurt_full, data = train)[, -1]
y_kurt <- train$log_kurtosis
lasso_kurt <- cv.glmnet(X_kurt_full, y_kurt, alpha = 1, nfolds = 10)
cv_lasso_kurt <- min(sqrt(lasso_kurt$cvm))
cat("LASSO (full) CV RMSE:", cv_lasso_kurt, "\n")
cat("Best lambda:", lasso_kurt$lambda.min, "\n")
cat("Non-zero coefficients:", sum(coef(lasso_kurt, s = "lambda.min") != 0) - 1, "\n")
```

## 5.5 Select Final Model

```{r kurt_final}
if(cv_ols_kurt < cv_lasso_kurt) {
  cat("FINAL MODEL: OLS on subset (lower CV RMSE)\n")
  final_model_kurt <- lm_kurt_best
  use_lasso_kurt <- FALSE
} else {
  cat("FINAL MODEL: LASSO on full model (lower CV RMSE)\n")
  final_model_kurt <- lasso_kurt
  use_lasso_kurt <- TRUE
}
```

---

# 6. Model Diagnostics

## 6.1 Residual Plots

```{r diagnostics_resid, fig.height=8, fig.width=10}
par(mfrow = c(2, 2))

plot(lm_mean_best, which = 1, main = "Mean: Residuals vs Fitted")
plot(lm_var_best, which = 1, main = "Variance: Residuals vs Fitted")
plot(lm_skew_best, which = 1, main = "Skewness: Residuals vs Fitted")
plot(lm_kurt_best, which = 1, main = "Kurtosis: Residuals vs Fitted")
```

The residual plot shows that the mean model satisfies key assumptions: residuals are centered around zero with no strong patterns, indicating a good linear fit. In contrast, the residual plots for variance, skewness, and kurtosis show clear curvature and changing spread, suggesting nonlinear relationships and heteroskedasticity. These patterns indicate that the current linear models may be misspecified and that additional transformations or nonlinear modeling approaches may be needed to improve performance for higher-order distribution statistics.

## 6.2 Q-Q Plots

```{r diagnostics_qq, fig.height=8, fig.width=10}
par(mfrow = c(2, 2))

plot(lm_mean_best, which = 2, main = "Mean: Q-Q Plot")
plot(lm_var_best, which = 2, main = "Variance: Q-Q Plot")
plot(lm_skew_best, which = 2, main = "Skewness: Q-Q Plot")
plot(lm_kurt_best, which = 2, main = "Kurtosis: Q-Q Plot")
```
The Q–Q plot for the mean model shows residuals close to normal, indicating the linear regression assumptions are well met. In contrast, the variance, skewness, and kurtosis models show substantial departures from normality, particularly in the tails, suggesting that these models may require more complex approaches to better capture the data patterns at the tails. (Same analysis as with residual plots)
 
---

# 7. Test Set Predictions

## 7.1 Load and Transform Test Data

```{r test_load}
test <- read_csv("data-test.csv")
```

```{r test_transform}
# Apply same transformations as training data
test <- test %>%
  mutate(
    Re = as.numeric(Re),
    Fr_invlogit = 1 / (1 + exp(-as.numeric(Fr))),
    log_St = log(St)
  )
```

## 7.2 Generate Predictions

```{r test_predict}
# predictions using the selected final models (OLS or LASSO)

# Mean predictions
if(use_lasso_mean) {
  # For LASSO, need design matrix from FULL model
  test_temp <- test %>% mutate(log_mean = 0)  # dummy response for model.matrix
  X_test_mean <- model.matrix(formula_mean_full, data = test_temp)[, -1]
  pred_mean_log <- predict(lasso_mean, newx = X_test_mean, s = "lambda.min")[,1]
} else {
  pred_mean_log <- predict(lm_mean_best, newdata = test)
}

# Variance predictions
if(use_lasso_var) {
  test_temp <- test %>% mutate(log_variance = 0)  # dummy response
  X_test_var <- model.matrix(formula_var_full, data = test_temp)[, -1]
  pred_var_log <- predict(lasso_var, newx = X_test_var, s = "lambda.min")[,1]
} else {
  pred_var_log <- predict(lm_var_best, newdata = test)
}

# Skewness predictions
if(use_lasso_skew) {
  test_temp <- test %>% mutate(log_skewness = 0)  # dummy response
  X_test_skew <- model.matrix(formula_skew_full, data = test_temp)[, -1]
  pred_skew_log <- predict(lasso_skew, newx = X_test_skew, s = "lambda.min")[,1]
} else {
  pred_skew_log <- predict(lm_skew_best, newdata = test)
}

# Kurtosis predictions
if(use_lasso_kurt) {
  test_temp <- test %>% mutate(log_kurtosis = 0)  # dummy response
  X_test_kurt <- model.matrix(formula_kurt_full, data = test_temp)[, -1]
  pred_kurt_log <- predict(lasso_kurt, newx = X_test_kurt, s = "lambda.min")[,1]
} else {
  pred_kurt_log <- predict(lm_kurt_best, newdata = test)
}

# Combine and back-transform from log scale
test_predictions <- test %>%
  mutate(
    pred_mean = exp(pred_mean_log),
    pred_variance = exp(pred_var_log),
    pred_skewness = exp(pred_skew_log),
    pred_kurtosis = exp(pred_kurt_log)
  ) %>%
  select(St, Re, Fr, pred_mean, pred_variance, pred_skewness, pred_kurtosis)

# display head
cat("First 10 predictions:\n")
print(test_predictions[1:10, ])
```

## 7.3 Confidence Intervals for Predictions

**Note:** Since Re only has 3 observed values (90, 224, 398) but is treated as continuous, there is substantial uncertainty in the relationship between Re and cluster statistics. Confidence intervals quantify this uncertainty.

```{r confidence_intervals}
# Generate 95% confidence intervals for OLS predictions
# (LASSO predictions don't have built-in CIs, so we use OLS models)

# Mean model CIs
if(!use_lasso_mean) {
  pred_mean_ci <- predict(lm_mean_best, newdata = test, interval = "confidence", level = 0.95)
  pred_mean_pi <- predict(lm_mean_best, newdata = test, interval = "prediction", level = 0.95)
  
  # Back-transform to original scale
  mean_intervals <- test %>%
    select(St, Re, Fr) %>%
    mutate(
      pred = exp(pred_mean_ci[, "fit"]),
      ci_lower = exp(pred_mean_ci[, "lwr"]),
      ci_upper = exp(pred_mean_ci[, "upr"]),
      pi_lower = exp(pred_mean_pi[, "lwr"]),
      pi_upper = exp(pred_mean_pi[, "upr"]),
      ci_width = ci_upper - ci_lower,
      pi_width = pi_upper - pi_lower
    )
  
  cat("Mean model - Average 95% CI width:", round(mean(mean_intervals$ci_width), 4), "\n")
  cat("Mean model - Average 95% PI width:", round(mean(mean_intervals$pi_width), 4), "\n\n")
  
  cat("First 5 predictions with intervals:\n")
  print(head(mean_intervals, 5))
}

# Interpretation:
# - Confidence Interval (CI): Uncertainty in the mean prediction
# - Prediction Interval (PI): Uncertainty for a new observation (wider)
# - wise intervals are expected since only3 observed Re values
```

## 7.4 Save Predictions

```{r test_save}
write_csv(test_predictions, "predictions.csv")
```

---

# 8. Model Summaries

## 8.1 Mean Model

```{r summary_mean}
cat("Mean model summary")
summary(lm_mean_best)
```

## 8.2 Variance Model

```{r summary_var}
cat("Variance model summary")
summary(lm_var_best)
```

## 8.3 Skewness Model

```{r summary_skew}
cat("Skewness Model summary")
summary(lm_skew_best)
```

## 8.4 Kurtosis Model

```{r summary_kurt}
cat("Kurtosis model summary")
summary(lm_kurt_best)
```

# 8.5 Cross-Validation RMSE Comparison

To compare predictive performance across all four models, we summarize the 10-fold CV RMSE below:

```{r cv_summary}
cv_results <- data.frame(
  Response = c("Mean", "Variance", "Skewness", "Kurtosis"),
  OLS_CV_RMSE = c(cv_ols_mean, cv_ols_var, cv_ols_skew, cv_ols_kurt),
  LASSO_CV_RMSE = c(cv_lasso_mean, cv_lasso_var, cv_lasso_skew, cv_lasso_kurt)
)

cv_results %>%
  mutate(
    Best_Method = ifelse(OLS_CV_RMSE < LASSO_CV_RMSE, "OLS", "LASSO"),
    Best_RMSE = pmin(OLS_CV_RMSE, LASSO_CV_RMSE)
  ) %>%
  arrange(Best_RMSE)
```

---

# 9. Key Findings

**Model Performance**:
- All models use log transformations to handle right skew
- Best subset selection identifies optimal predictor combinations (OLS approach)
- LASSO on full model performs automatic variable selection via regularization
- Cross-validation determines which approach (subset+OLS vs LASSO) works best per response

**What I think we should do next**
1. Examine coefficient signs and magnitudes
2. Identify which predictors are most important for each response
3. Look for consistent patterns across all 4 models
4. Interpret interaction effects in physical terms
5. Compare CV RMSE across models to see which responses are easiest/hardest to predict

